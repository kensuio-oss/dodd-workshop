{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4a6cf3-acc0-4f42-b54c-32881d4c9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/05 10:59:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"libs/kensu-dam-spark-collector-0.17.2_spark-3.0.1.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "927b12a7-ccd6-4ac5-9497-afefd130b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kensuspark import init_kensu_DAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513ec185-305d-44d3-935b-5fd3ed52d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = \"https://li-demo2107-he-api.464n.com\"\n",
    "\n",
    "token = \"eyJhbGciOiJIUzI1NiJ9.eyIkaW50X3Blcm1zIjpbXSwic3ViIjoib3JnLnBhYzRqLmNvcmUucHJvZmlsZS5Db21tb25Qcm9maWxlI3NhbW15IiwidG9rZW5faWQiOiI3YmY3NTU4OS0xZTQwLTQxYzgtODI2Yi0xNjIyZDQ0NDc0OGIiLCJhcHBncm91cF9pZCI6IjE2YjI5YWFmLWEyNzItNDBhNi05NTZkLWI4MGM2MzYxZDM2OSIsIiRpbnRfcm9sZXMiOlsiYXBwIl0sImV4cCI6MTk1MTQ2OTE4MSwiaWF0IjoxNjM2MTA5MTgxfQ.exoCAJp6fbrEXAJXJQtQTQAjygJuY744UmvjQ6msgg8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae2ebcb9-0559-45d7-bb7a-af44a2d86601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DAM...\n",
      "Notebook name Untitled.ipynb\n",
      "maybeDamFileDebugLevel: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/05 10:59:33 WARN Implicits$: DAM: Spark version: 3.0.1\n",
      "21/11/05 10:59:33 WARN Implicits$: DAM: DAM Spark collector build info: { assembly_assemblyJarName: kensu-dam-spark-collector-0.17.2_spark-3.0.1.jar, version: 0.17.2, scalaVersion: 2.12.10, gitHeadCommit: Some(076db20d204e47f3ed55c74668207ec8ceea06f9), builtAtString: 2021-06-01 11:23:24.562+0300, builtAtMillis: 1622535804562 }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new DAM scala client actor-system\n",
      "Done creating a new DAM scala client actor-system.\n",
      "patching spark.stop to wait for DAM reporting to finish\n",
      "patching spark.stop done\n",
      "Adding DataFrame.report_as_dam_datasource\n",
      "done adding DataFrame.report_as_dam_datasource\n",
      "Adding DataFrame.report_as_dam_jdbc_datasource\n",
      "done adding DataFrame.report_as_dam_jdbc_datasource\n",
      "Adding DataFrame.report_as_kpi\n",
      "done adding DataFrame.report_as_kpi\n",
      "patching DataFrame.toPandas\n",
      "done patching DataFrame.toPandas\n",
      "patching spark.createDataFrame to work with pandas dataframes\n",
      "patching  spark.createDataFrame done\n",
      "adding spark env var spark.executorEnv.KSU_DISABLE_PY_COLLECTOR=true to disable kensu-py collector on executor nodes\n",
      "done adding spark.executorEnv.KSU_DISABLE_PY_COLLECTOR\n"
     ]
    }
   ],
   "source": [
    "init_kensu_DAM(spark,\n",
    "               ingestion_url=api,ingestion_token=token,\n",
    "               is_offline=False,\n",
    "               project='Test',\n",
    "               environment=\"Lab\",\n",
    "               logical_datasource_name_strategy='File')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a2d677-16cc-4baf-8669-ac705601ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features = spark.read.option(\"inferschema\",\"true\").csv('demo/week1/custinfo.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9e482c-7ed1-4d5f-ad3d-76487c5e83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/05 11:00:10 WARN EnvironnementProvider$: DAM: Creating EnvironnementProvider io.kensu.dam.lineage.spark.lineage.DefaultSparkEnvironnementProvider\n",
      "21/11/05 11:00:10 WARN DAMDataLineageWriter: DAM: inputDatasourceOverrides: Map()\n",
      "21/11/05 11:00:10 WARN DAMDataLineageWriter: DAM: outputDatasourceOverride: None\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features.write.save('testpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56308662-c6ba-4f23-b36f-7e37e39ffbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d7fb0-d3e6-4d5c-863d-57608ff946e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
