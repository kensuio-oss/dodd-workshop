{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kensu initialisation\n",
    "\n",
    "To import the library, you need to add the kensu prefix to the library we need to monkeypatch:\n",
    "\n",
    "$$\\text{pandas} \\rightarrow  \\text{kensu.pandas}$$\n",
    "\n",
    "Once imported, you can initialize the client with the `KensuProvider` object. Several parameters are available and you can find the list in the user documentation. \n",
    "\n",
    "The `Context` of the application is defined by its `process_name` (an identifier for your application), `project_name`(where the application is running), and `environment`.\n",
    "\n",
    "`execution_timestamp` allows to define the execution timestamp of the notebook, and is set for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kensu.utils.kensu_provider import KensuProvider\n",
    "application1 = 'Join Data'\n",
    "application2 = 'Compute Total'\n",
    "project = 'DODD'\n",
    "env = 'Lab'\n",
    "user = os.getenv('USER')\n",
    "ingestion_url = \"\"\n",
    "ingestion_token = \"\"\n",
    "api_token = \"\"\n",
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application1,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,10,25).timestamp())*1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import kensu.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1635112800000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "round(datetime.datetime(2021,10,25).timestamp())*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execution of the pipeline - 25/10/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = pd.read_csv('./demo/week1/custinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('./demo/week1/transactions.csv',parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_customer.merge(df_transaction,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./demo/week1/joined.csv',index = False)\n",
    "\n",
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application2,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,10,25).timestamp())*1000)\n",
    "\n",
    "df = pd.read_csv('./demo/week1/joined.csv',parse_dates = ['date'])\n",
    "df['Total'] = df['price']*df['quantity']\n",
    "df.to_csv('./demo/week1/data_with_total.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execution of the pipeline - 30/10/2021\n",
    "\n",
    "**Note:** we reinitialise the client in order to take the new timestamp into account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application1,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,10,30).timestamp()*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = pd.read_csv('./demo/week2/custinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('./demo/week2/transactions.csv',parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_customer.merge(df_transaction,on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./demo/week2/joined.csv',index = False)\n",
    "\n",
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application2,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,10,30).timestamp())*1000)\n",
    "\n",
    "df = pd.read_csv('./demo/week2/joined.csv',parse_dates = ['date'])\n",
    "df['Total'] = df['price']*df['quantity']\n",
    "df.to_csv('./demo/week2/data_with_total.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Execution of the pipeline - 06/11/2021\n",
    "\n",
    "**Note:** we reinitialise the client in order to take the new timestamp into account "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application1,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,11,6).timestamp())*1000,kensu_api_token=api_token)\n",
    "\n",
    "import kensu.pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = pd.read_csv('./demo/week3/custinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kensu.utils.rule_engine import add_missing_value_rules\n",
    "add_missing_value_rules('custinfo.csv',df_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction = pd.read_csv('./demo/week3/transactions.csv',parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_customer.merge(df_transaction, on ='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./demo/week3/joined.csv',index = False)\n",
    "\n",
    "k = KensuProvider().initKensu(kensu_ingestion_url= ingestion_url,\n",
    "                              kensu_ingestion_token = ingestion_token,\n",
    "                              process_name= application2,\n",
    "                              user_name= user, code_location='https://gitlab.example.com',\n",
    "                              project_name=project, environment= env, pandas_support=True, sklearn_support=False,\n",
    "                              tensorflow_support=False, bigquery_support=False,pyspark_support=False,logical_data_source_naming_strategy = 'File',\n",
    "                              allow_reinit = True, execution_timestamp = round(datetime.datetime(2021,11,6).timestamp())*1000)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('./demo/week3/joined.csv',parse_dates = ['date'])\n",
    "df['Total'] = df['price']*df['quantity']\n",
    "df.to_csv('./demo/week3/data_with_total.csv',index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
