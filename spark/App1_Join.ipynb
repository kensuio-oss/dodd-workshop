{"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"spylon-kernel","language":"scala","name":"spylon-kernel"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","help_links":[{"text":"MetaKernel Magics","url":"https://metakernel.readthedocs.io/en/latest/source/README.html"}],"mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"0.4.1"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Kensu initialisation\n\nTo use the library, you need to add the kensu jar to the spark client\n\nInitialize the client with the `KensuProvider` object with its `Context`:\n- `process_name`:  the application name\n- `project_names`: where the application is running \n- `environment`: ... well","metadata":{},"id":"75e18105"},{"cell_type":"code","source":"%%init_spark\nlauncher.jars = [\"libs/kensu-dam-spark-collector-0.17.2_spark-3.0.1.jar\",\"libs/sdk_2.12.jar\"]\nlauncher.conf.set(\"spark.sql.shuffle.partitions\", \"1\")","metadata":{"trusted":true},"execution_count":1,"outputs":[],"id":"ef3fc5c6"},{"cell_type":"code","source":"val app = 1\nval week = 1","metadata":{"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Intitializing Scala interpreter ..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Spark Web UI available at http://jupyter-kensuio-2doss-2ddodd-2dworkshop-2dr10bzxei:4040\nSparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1636438800758)\nSparkSession available as 'spark'\n"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"app: Int = 1\nweek: Int = 1\n"},"metadata":{}}],"id":"aeabb3b0"},{"cell_type":"code","source":"implicit val ch = new io.kensu.dodd.sdk.ConnectHelper(s\"./conf/application${app}-week${week}.properties\")\nval dataFolder = \"data\"+ch.properties(\"dam.activity.user\").toString\nio.kensu.third.integration.TimeUtils.setMockedTime(ch.properties(\"mocked.timestamp\").asInstanceOf[Long], false)\nio.kensu.dam.lineage.spark.lineage.Implicits.SparkSessionDAMWrapper(spark).track(ch.properties.get(\"dam.ingestion.url\").map(_.toString), None)(ch.properties.toList:_*)","metadata":{"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"maybeDamFileDebugLevel: Some(INFO)\nActivating DAM debug logging to file: debug.log\nDone activating DAM debug logging to a file\nCreating a new DAM scala client actor-system\nDone creating a new DAM scala client actor-system.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"ch: io.kensu.dodd.sdk.ConnectHelper = io.kensu.dodd.sdk.ConnectHelper@53517333\ndataFolder: String = datamyname\nres0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@b9c5fc9\n"},"metadata":{}}],"id":"3a9579bd"},{"cell_type":"markdown","source":" ## 2. Execution of the pipeline - App 1 : Join\n \n This application is joining datasource `customer` and `transaction`\n","metadata":{},"id":"f0c6c6c6"},{"cell_type":"code","source":"val df_customer = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(s\"./data_${ch.properties(\"dam.activity.user\")}/week${week}/custinfo\")","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"df_customer: org.apache.spark.sql.DataFrame = [id: int, first_name: string ... 3 more fields]\n"},"metadata":{}}],"id":"d15c7583"},{"cell_type":"code","source":"val df_transaction = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(s\"./data_${ch.properties(\"dam.activity.user\")}/week${week}/transactions\")","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"df_transaction: org.apache.spark.sql.DataFrame = [id: int, date: double ... 3 more fields]\n"},"metadata":{}}],"id":"41178b2b"},{"cell_type":"code","source":"val df_join = df_customer.join(df_transaction, df_customer(\"id\") === df_transaction(\"id\"),\"inner\").drop(df_transaction(\"id\"))","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"df_join: org.apache.spark.sql.DataFrame = [id: int, first_name: string ... 7 more fields]\n"},"metadata":{}}],"id":"90394264"},{"cell_type":"code","source":"df_join.count()","metadata":{"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"res1: Long = 1000\n"},"metadata":{}}],"id":"1793cd4a"},{"cell_type":"code","source":"val sdk = new io.kensu.dodd.sdk.SDK(ch.properties(\"kensu.sdk.url\").toString, ch.properties(\"kensu.pat\").toString)","metadata":{"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"sdk: io.kensu.dodd.sdk.SDK = io.kensu.dodd.sdk.SDK@4884e416\n"},"metadata":{}}],"id":"e8269d25"},{"cell_type":"code","source":"sdk.range()","metadata":{},"execution_count":null,"outputs":[],"id":"345e80f1-43dc-4708-b502-bc82f100623b"},{"cell_type":"code","source":"df_join.write.mode(\"overwrite\").save(s\"./data_${properties(\"dam.activity.user\")}/week${week}/joined_data\")","metadata":{},"execution_count":null,"outputs":[],"id":"b2751c64"}]}