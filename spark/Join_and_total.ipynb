{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.jars = [\"libs/kensu-dam-spark-collector-0.17.2_spark-3.0.1.jar\"]\n",
    "launcher.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://jupyter-kensuio-2doss-2ddodd-2dworkshop-2d6frk2s67:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1636124412003)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maybeDamFileDebugLevel: Some(DEBUG)\n",
      "Activating DAM debug logging to file: debug.log\n",
      "Done activating DAM debug logging to a file\n",
      "Creating a new DAM scala client actor-system\n",
      "Done creating a new DAM scala client actor-system.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "properties: scala.collection.mutable.Map[String,Any] = Map(dam.activity.spark.environnement.provider -> io.kensu.dam.lineage.spark.lineage.DefaultSparkEnvironnementProvider, dam.activity.code.repository -> binder://workshop-dodd, dam.activity.organization -> Unknown, dam.activity.explicit.process.name -> taratata, dam.spark.data_stats.input.only_used_in_lineage -> true, dam.spark.file_debug.file_name -> debug.log, dam.datasources.path_rules.short.naming_strategy -> \"\", dam.activity.explicit.process_run.name -> taratata, dam.spark.data_stats.enabled -> true, dam.spark.data_stats.output.coalesceEnabled -> false, dam.ingestion.is_offline -> true, dam.ingestion.entity.compaction -> true, dam.activity.user -> jovyan, dam.ingestion.auth.token -> eyJhbGciOiJIUzI1NiJ9.eyIkaW50X3Blcm1zIjpbXSwic3...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val properties = {\n",
    "    import scala.collection.JavaConverters._\n",
    "    val p = new java.util.Properties()\n",
    "    p.load(new java.io.FileInputStream(\"./application.properties\"))\n",
    "    val m = scala.collection.mutable.Map.empty[String, Any]\n",
    "    m++=p.asScala.mapValues(v => scala.util.Try(v.toBoolean).orElse(scala.util.Try(v.toInt)).getOrElse(v))\n",
    "    m\n",
    "}\n",
    "val dam_url = \"\"\n",
    "val token = \"\"\n",
    "val processName = \"taratata\"\n",
    "val mocked_timestamp = System.currentTimeMillis\n",
    "\n",
    "properties.put(\"dam.ingestion.auth.token\", token)\n",
    "properties.put(\"dam.activity.code.maintainers\", List(sys.env(\"USER\")))\n",
    "properties.put(\"dam.activity.code.version\", \"\"+System.currentTimeMillis)\n",
    "properties.put(\"dam.activity.user\", sys.env(\"USER\"))\n",
    "properties.put(\"dam.activity.organization\", \"Unknown\")\n",
    "properties.put(\"dam.activity.projects\", properties(\"dam.activity.projects\").toString.split(\",\"))\n",
    "properties.put(\"dam.activity.explicit.process.name\", processName)\n",
    "properties.put(\"dam.activity.explicit.process_run.name\", processName)\n",
    "\n",
    "\n",
    "io.kensu.third.integration.TimeUtils.setMockedTime(mocked_timestamp, false)\n",
    "\n",
    "val w = io.kensu.dam.lineage.spark.lineage.Implicits.SparkSessionDAMWrapper(spark)\n",
    "w.track(Some(dam_url), None)(properties.toList:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "week: String = week1\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val week = \"week1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_customer: org.apache.spark.sql.DataFrame = [id: int, first_name: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_customer = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(s\"./data/${week}/custinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_transaction: org.apache.spark.sql.DataFrame = [id: int, date: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_transaction = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(s\"./data/${week}/transactions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_join: org.apache.spark.sql.DataFrame = [id: int, first_name: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_join = df_customer.join(df_transaction, df_customer(\"id\") === df_transaction(\"id\"),\"inner\").drop(df_transaction(\"id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Long = 1000\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join.write.mode(\"overwrite\").save(s\"./data/${week}/joined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [id: int, first_name: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(s\"./data/${week}/joined_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+--------------------+-----------+----------+--------------------+-----+--------+\n",
      "| id|first_name|  last_name|               email|     gender|      date|             product|price|quantity|\n",
      "+---+----------+-----------+--------------------+-----------+----------+--------------------+-----+--------+\n",
      "|  1|    Halsey|   Andreone|handreone0@printf...|    Agender|10/24/2021|Lettuce - Boston ...|  8.8|       1|\n",
      "|  2|       Web|  Fountaine|wfountaine1@twitp...|    Agender|10/21/2021|Wine - Cave Sprin...| 10.2|       3|\n",
      "|  3|      Pepe|   Skoggins|pskoggins2@diigo.com|     Female|10/18/2021|Cookie Dough - Pe...| 10.9|       1|\n",
      "|  4|    Goldie|     Chevis|gchevis3@hatena.n...|Genderfluid|10/19/2021|Triple Sec - Mcgu...| 14.2|       5|\n",
      "|  5|      Joli|Matuszewski|jmatuszewski4@4sh...| Non-binary|10/23/2021|Beer - Sleemans H...| 16.8|       5|\n",
      "|  6|    Crysta|     Wybern|cwybern5@eventbri...|     Female|10/18/2021|Syrup - Kahlua Ch...|  8.1|       5|\n",
      "|  7|      Evyn|    Mulrean|  emulrean6@sohu.com| Non-binary|10/18/2021|Cake - Cake Sheet...| 18.0|       3|\n",
      "|  8|    Edgard|    Kingdon|   ekingdon7@soup.io|   Bigender|10/18/2021|   Cherries - Frozen|  5.5|       2|\n",
      "|  9|     Thoma|    Casburn|   tcasburn8@nyu.edu|   Bigender|10/22/2021|    Devonshire Cream| 15.9|       2|\n",
      "| 10|    Gwynne|     Chiene|  gchiene9@google.ru|       Male|10/22/2021|          Pur Source|  7.9|       3|\n",
      "| 11|    Nellie| MacDermand|nmacdermanda@accu...| Polygender|10/21/2021|       Leeks - Large| 12.2|       4|\n",
      "| 12|  Mordecai|    Tinston|   mtinstonb@last.fm|     Female|10/18/2021|Tea - Apple Green...|  1.6|       4|\n",
      "| 13|  Genovera| Woodthorpe|gwoodthorpec@spie...| Non-binary|10/20/2021|   Artichoke - Fresh|  1.0|       1|\n",
      "| 14|    Wallie| Daskiewicz|wdaskiewiczd@wash...|Genderqueer|10/24/2021|      Beets - Golden|  2.6|       2|\n",
      "| 15|    Darrel|     Pigney|dpigneye@chicagot...|Genderqueer|10/18/2021|Oneshot Automatic...| 16.1|       2|\n",
      "| 16|     Marta|   Harfleet|mharfleetf@seesaa...| Non-binary|10/19/2021|   Veal - Sweetbread| 14.0|       5|\n",
      "| 17|   Josepha|    Renvoys|     jrenvoysg@ow.ly|    Agender|10/21/2021|Latex Rubber Glov...| 13.0|       3|\n",
      "| 18|    Ardath|       Tebb|  atebbh@storify.com|     Female|10/24/2021|Trueblue - Bluebe...| 10.4|       5|\n",
      "| 19|    Tonnie| Thunderman|tthundermani@blog...|       Male|10/22/2021|          Pea - Snow| 13.1|       1|\n",
      "| 20|      Glen|      Cerie| gceriej@pcworld.com| Non-binary|10/23/2021|    Pork - Side Ribs| 14.4|       3|\n",
      "+---+----------+-----------+--------------------+-----------+----------+--------------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Total_DF: org.apache.spark.sql.DataFrame = [id: int, price: double ... 2 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Total_DF = spark.sql(\"SELECT id,price,quantity,round(price * quantity,2) AS total FROM df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+-----+\n",
      "| id|price|quantity|total|\n",
      "+---+-----+--------+-----+\n",
      "|  1|  8.8|       1|  8.8|\n",
      "|  2| 10.2|       3| 30.6|\n",
      "|  3| 10.9|       1| 10.9|\n",
      "|  4| 14.2|       5| 71.0|\n",
      "|  5| 16.8|       5| 84.0|\n",
      "|  6|  8.1|       5| 40.5|\n",
      "|  7| 18.0|       3| 54.0|\n",
      "|  8|  5.5|       2| 11.0|\n",
      "|  9| 15.9|       2| 31.8|\n",
      "| 10|  7.9|       3| 23.7|\n",
      "| 11| 12.2|       4| 48.8|\n",
      "| 12|  1.6|       4|  6.4|\n",
      "| 13|  1.0|       1|  1.0|\n",
      "| 14|  2.6|       2|  5.2|\n",
      "| 15| 16.1|       2| 32.2|\n",
      "| 16| 14.0|       5| 70.0|\n",
      "| 17| 13.0|       3| 39.0|\n",
      "| 18| 10.4|       5| 52.0|\n",
      "| 19| 13.1|       1| 13.1|\n",
      "| 20| 14.4|       3| 43.2|\n",
      "+---+-----+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Total_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_DF.write.mode(\"overwrite\").save(s\"./data/${week}/data_total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
